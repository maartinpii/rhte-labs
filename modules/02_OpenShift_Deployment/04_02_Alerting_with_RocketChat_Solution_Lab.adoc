:scrollbar:
:data-uri:
:toc2:
:linkattrs:
:course_name: Red Hat OpenShift Operations


==  Rocket.Chat Alerts Lab

Rocket.Chat is an open source chat platform similar to Slack. You can easily deploy it on top of OpenShift and use it as an example of how to receive alerts from Prometheus in case things go wrong.

:numbered:


==== Preliminary Important Notes

. *Performance*: The lab environment is cloudased, so you can access it over
 the WAN from anywhere. However, do not expect its performance to match a
  bare-metal environment.

. *Remote Access*: The bastion host is the only host that you can access with
 SSH outside the lab environment. External SSH for all other hosts is blocked.
  From the bastion host, you can access the other hosts internally through SSH.
   As described earlier, you must access the system (not as `root`) with your
    private SSH key and OPENTLC login.

. *GUID*: Each lab environment is assigned a global unique identifier (GUID)
 with four characters, which you receive by email when provisioning your lab
  environment. *From this point on, replace _GUID_ with your lab's four-character GUID.*

. *Bastion host* is *not* an OpenShift cluster member or part of the OpenShift
 environment. The bastion host mimics your client's infrastructure or your
  laptop or desktop that is connected to the client's local area network (LAN).
+
[TIP]
It is recommended to use a terminal multiplexing tool, such as
 *tmux* or *screen*, which keeps your place in the session if you are
  disconnected from your environment. You can install their packages after
   setting up the `rhel` repositories `yum install tmux`.
If you use tmux, type *Ctrl+B* (to enter "scroll mode") + page up or down to
 scroll, and use the *Esc* key to exit scroll mode.
You can detach from tmux : `Ctrl+B  D` or simply close you terminal. To attach
 again an existing tmux session, run the command `tmux attach` once you're
  connected to the bastion host.
+
. Provisioned Environment Hosts

These are the hosts that have been deployed in your lab environment:

* Bastion (administration) server: `bastion.GUID.example.opentlc.com`, `bastion.GUID.internal`
* NFS server: `support1.GUID.example.opentlc.com`, `support1.GUID.internal`
* Load balancer: `loadbalancer.GUID.example.opentlc.com`, `loadbalancer1.GUID.internal`
* 3 OpenShift master nodes: `master{1,2,3}.GUID.example.opentlc.com`, `master{1,2,3}.GUID.internal`
* 2 OpenShift infrastructure nodes: `infranode{1,2}.GUID.example.opentlc.com`, `infranode{1,2}.GUID.internal`
* 1 OpenShift worker node: `node1.GUID.example.opentlc.com`, `node1.GUID.internal`
* IPA Server: `ipa.shared.example.opentlc.com` (shared resource for all students)

==== Connect to Bastion Host and Explore your environment

When you connect to `bastion.GUID.example.opentlc.com` for the first time, you
will have to accept the server SSH fingerprint. Reply 'yes': it will be added
 to your `known_hosts` and not asked next time you connect.

. Connect to your administration host `bastion.GUID.example.opentlc.com`. Note that your private key location may vary.
+
[source,bash]
----
yourdesktop$ ssh -i ~/.ssh/studentkey ec2-user@bastion.GUID.example.opentlc.com
----
+
* Example of a successful connection:
+
[source,bash]
----
[gucore@work ~]$ ssh -i ~/.ssh/studentkey ec2-user@bastion.9e91.example.opentlc.com
----
+
[source,text]
----
The authenticity of host 'bastion.9e91.example.opentlc.com (31.220.66.121)' can't be established.
ECDSA key fingerprint is SHA256:fR4vFVswyvpj/Jevfin3+X0Fkehbfx4HBjw46AeIS14.
ECDSA key fingerprint is MD5:bb:25:92:ee:c9:ba:23:71:b7:c1:f7:2d:89:6d:b0:66.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'bastion.9e91.example.opentlc.com,31.220.66.121' (ECDSA) to the list of known hosts.
Creating home directory for ec2-user.
[ec2-userbastion.9e91 ~]$
----

. Run `sudo -i` to become the `root` user on the bastion host:
+
[source,bash]
----
sudo -i
----
+
. Ensure that you have a GUID environment variable set to make entering commands
easier
+
[source,bash]
----
echo ${GUID}
----
+
The output should be your GUID.
+
If the output is NOT your GUID, execute the following to set your unique identifier as the GUID environment variable:
+
[source,bash]
----
export GUID=$(hostname | cut -d. -f2)
----
+
Validate that it is set, and add it to your profile.
+
[source,bash]
----
echo ${GUID}; echo "export GUID=${GUID}" >> /root/.bashrc
----


. A sample Ansible inventory file with host in your environment has been created
 for you, have a look at `/etc/ansible/hosts`
+
[source,bash]
----
cat /etc/ansible/hosts
----
+
. Use the Ansible `--list-hosts` command line to list the masters, nodes, and
 all of the host groups:
+
.. List the "masters" host group:
+
[source,bash]
----
ansible masters --list-hosts
----
+
Expect the output to look similar to this:
+
[source,text]
----
  hosts (3):
    master1.GUID.internal
    master2.GUID.internal
    master3.GUID.internal
----
+
.. List the "nodes" host group (Remember, Masters are Nodes too):
+
[source,bash]
----
ansible nodes --list-hosts
----
+
Expect the output to look similar to this:
+
[source,bash]
----
hosts (8):
    master1.GUID.internal
    master2.GUID.internal
    master3.GUID.internal
    infranode1.GUID.internal
    infranode2.GUID.internal
    node1.GUID.internal
    node2.GUID.internal
----
+
.. List the "all" host group:
+
[source,bash]
----
ansible all --list-hosts
----
+
Expect the output to look similar to this:
+
[source,text]
----
hosts (10):
    master1.GUID.internal
    master2.GUID.internal
    master3.GUID.internal
    infranode1.GUID.internal
    infranode2.GUID.internal
    node1.GUID.internal
    node2.GUID.internal
    loadbalancer1.GUID.internal
    support1.GUID.internal
----
+
. Test the Ansible configuration by using the Ansible "ping" module to contact all
the hosts.  This also ensures that all the hosts are running.:
+
[source,bash]
----
ansible all -m ping
----
+
Expect the output to look similar to this:
+
[source,text]
----
loadbalancer1.GUID.internal | SUCCESS => {
    "changed": false,
    "failed": false,
    "ping": "pong"
}
infranode1.GUID.internal | SUCCESS => {
    "changed": false,
    "failed": false,
    "ping": "pong"
}
master2.GUID.internal | SUCCESS => {
    "changed": false,
    "failed": false,
    "ping": "pong"
}
master3.GUID.internal | SUCCESS => {
    "changed": false,
    "failed": false,
    "ping": "pong"
}
master1.GUID.internal | SUCCESS => {
    "changed": false,
    "failed": false,
    "ping": "pong"
}
infranode2.GUID.internal | SUCCESS => {
    "changed": false,
    "failed": false,
    "ping": "pong"
}
node1.GUID.internal | SUCCESS => {
    "changed": false,
    "failed": false,
    "ping": "pong"
}
node2.GUID.internal | SUCCESS => {
    "changed": false,
    "failed": false,
    "ping": "pong"
}
support1.GUID.internal | SUCCESS => {
    "changed": false,
    "failed": false,
    "ping": "pong"
}
node3.GUID.internal | SUCCESS => {
    "changed": false,
    "failed": false,
    "ping": "pong"
}
----



== Set Up Rocket.Chat

. Log in again to the shell on your Bastion host and switch to the `root` user.

. Create a new project to hold the Rocket.Chat instance:
+
[source,bash]
----
oc new-project rocketchat --display-name="Rocket Chat"
----

. Deploy a persistent MongoDB database to hold the Rocket.Chat configuration:
+
[source,bash]
----
oc new-app mongodb-persistent -p MONGODB_USER=rocketchat-admin -p MONGODB_PASSWORD=rocketchat -p MONGODB_DATABASE=rocketchat -p VOLUME_CAPACITY=4Gi
----

. Wait until the MongoDB pod is fully initialized.

. Deploy the Rocket Chat image from Docker Hub, connecting it to the MongoDB database that you just created, and expose the Rocket Chat service as a route:
+
[source,bash]
----
oc new-app docker.io/rocketchat/rocket.chat:0.59.1 -e MONGO_URL=mongodb://rocketchat-admin:rocketchat@mongodb:27017/rocketchat
oc expose svc rocketchat
----

. Monitor your pods and wait until Rocket Chat fully starts.
* Tail the pod logs to make sure it starts successfully.

. Open a `rsh` session into your MongoDB pod:
+
[source,bash]
----
oc rsh <mongodb_pod>
----
+
[TIP]
Use `oc get pod` to find the specific name of your MondoDB pod--expect it to be similar to `mongodb-1-mgvb2`.

. Once inside your MongoDB pod, connect to the database:
+
[source,bash]
----
mongo localhost:27017
----

. Enter the following commands one by one:
+
[source,bash]
----
use rocketchat
db.auth('rocketchat-admin','rocketchat')
db.rocketchat_settings.update({_id:'Accounts_UseDNSDomainCheck'},{$set:{value:false}})
exit
----
* Rocket.Chat uses a domain check code to verify the validity of the email
 address. Because you do not have any email providers available in the lab, you
  disable this check.

. Type `exit` and then press `Enter` to disconnect from your MondoDB pod.


== Configure Rocket.Chat

=== Connect to Rocket.Chat and Register Account

In this section, you connect to Rocket.Chat and register a new account.

. Determine the URL of your Rocket.Chat instance:
+
[source,bash]
----
oc get route rocketchat
----

. In a web browser, navigate to the Rocket.Chat URL.

. Under the blue *Login* button, select *Register a new account*.

. Enter your details (Name, e-mail address and pick a password).

. Click *Register A New Account*.

. On the warning dialog, click *Yes*.

. When prompted to register the username, update the suggestion or keep the default, and click *Use this username*.


=== Create Channel in Rocket.Chat

In this section, you create a channel in Rocket.Chat so that Alertmanager has a destination for sending alerts.

. Next to the *Search* box under your name click *+*.

. Use `openshift-alerts` as the channel name and click *Create*.

=== Create Inbound Webhook

In this section, you create the inbound webhook for the new channel to receive alerts from Alertmanager.

. Next to your username, click the three dots and select *Administration*.

. Select *Integrations*, then *New Integration*, and then *Incoming WebHook*.

. Enter or select the following values:
* *Enabled*: `True`
* *Name*: `OpenShift Alerts`
* *Post to Channel*: `#openshift-alerts`
* *Post as*: `rocket.cat`
* *Alias*: `Prometheus`
* *Script Enabled*: `True`

. Copy and paste the script below into the `Script` box:
+
[source,javascript]
----
class Script {
  process_incoming_request({
    request
  }) {
    var alertColor = "warning";
    var alertKind  = "UNDEFINED";

    if (request.content.status == "resolved") {
      alertColor = "good";
      alertKind  = "RESOLVED"
    } else if (request.content.status == "firing") {
      alertColor = "danger";
      alertKind  = "PROBLEM"
    }
    console.log(request.content);

    let finFields = [];
    for (i = 0; i < request.content.alerts.length; i++) {
      var endVal = request.content.alerts[i];
      var elem = {
        title: alertKind + ": " + endVal.labels.alertname,
        value: "Instance: " + endVal.labels.instance + "; " + "Description: " + endVal.annotations.description + "; " + "Summary: " + endVal.annotations.summary,
        short: false
      };

      finFields.push(elem);
    }
    return {
      content: {
        username: "Alertmanager",
        attachments: [{
          color: alertColor,
          title_link: request.content.externalURL,
          title: "Prometheus notification",
          fields: finFields,
        }]
      }
    };

    return {
      error: {
        success: false,
        message: 'Error accepting Web Hook'
      }
    };
  }
}
----
+
. Click *Save Changes*.

. Copy the webhook URL--you need it later to configure Alertmanager.
* This field appears only after clicking *Save Changes*.


. Next to *Administration*, click the *x* and return to the chat window.

. From a shell prompt, test that your webhook is working by sending an example JSON payload to Rocket.Chat, replacing `<WEBHOOK_URL>` with the URL you copied down earlier:
+
[source,bash]
----
curl -X POST -H 'Content-Type: application/json' --data '{"alerts": [{"status": "testing", "labels": {"alertname": "alert_test", "instance": "instance.my.test.cluster" },   "annotations": { "description": "Alert Test Description",      "summary": "Alert Test Summary" } }]}' <WEBHOOK_URL>
----
* Expect to receive an alert message in your Rocket.Chat channel.


== Install Alertmanager

Now that you have a target to receive your alerts, you configure Alertmanager to send alerts when things go wrong.

. On your Bastion host, change to the directory with the Alertmanager code and switch projects to the `prometheus` OpenShift project:
+
[source,bash]
----
cd /root/openshift-prometheus/alertmanager
oc project prometheus
----

. Create Alertmanager, replacing `<WEBHOOK_URL>` with the previously copied webhook URL:
+
[source,bash]
----
oc new-app -f alertmanager.yaml -p VOLUME_CAPACITY=4Gi -p WEBHOOK_URL=<WEBHOOK_URL>
----

. You can see the configuration of Alertmanager by examining the configuration map created by the template:
+
[source,bash]
----
oc get cm alertmanager -o yaml
----

. Examine the end of the file to see the section that configures Alertmanager to send the alert to a webhook:
+
[source,yaml]
----
receivers:
- name: 'webhook'
  webhook_configs:
  - send_resolved: true
    url: '<WEBHOOK_URL'
----
+
[TIP]
If you want to use other notification mechanisms such as email, PagerDuty, or Slack, you update the configuration map with the correct configuration, as documented in link:https://prometheus.io/docs/alerting/configuration["https://prometheus.io/docs/alerting/configuration^"]).


== Test Alerting

In this section, you test the alerting to make sure it works.

. From your Bastion host, remotely connect using SSH into one of the nodes.

. Shut down the `atomic-openshift-node` service:
+
[source,bash]
----
systemctl stop atomic-openshift-node
----
+
[NOTE]
You may want to make sure that this node is not running either Alertmanager or the Prometheus pod. You can use `oc get pod -n project -o wide` to display which pods run on which nodes. Your best bet probably is to shut down an infranode rather than a worker node, because the Rocket.Chat and Prometheus pods are usually distributed to regular nodes.

* After a minute or two, expect to see an alert with a red bar and some descriptive text in the Rocket.Chat window.

* You can also see the alert in both the Prometheus and Alertmanager user interfaces.

. Bring the node back online:
+
[source,bash]
----
systemctl start atomic-openshift-node
----
* A few minutes later, expect to see a second notification with the same text, but with a green bar indicating that the situation is resolved.


== Clean Up

. If you want to remove your monitoring environment, delete the `prometheus` OpenShift project and the `prometheus-cluster-reader` cluster role-binding:
+
[source,bash]
----
oc delete project prometheus
oc delete clusterrolebinding prometheus-cluster-reader
----
